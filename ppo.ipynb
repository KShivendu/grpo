{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from typing import List, Tuple\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAndValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, policy_output_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, policy_output_dim)\n",
    "        )\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def value(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Takes in observation/state and returns value (based on past rewards) of being in the given observation/state\"\"\"\n",
    "        z = self.shared_layers(state)\n",
    "        value = self.value_layers(z)\n",
    "        return value # single number\n",
    "\n",
    "    def policy(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Takes in observation/state and returns logits for different actions\n",
    "\n",
    "        Note: Take `e^logits`, sum for all actions, P(obs, action) = e^logits / sum\n",
    "        \"\"\"\n",
    "        z = self.shared_layers(state)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits # (1, action_space_size)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        z = self.shared_layers(state)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n",
    "\n",
    "    def get_action_and_value(self, state: List[float]) -> Tuple[Tuple[int, float], float]:\n",
    "        \"\"\"Return ((action, action_log_prob, action_prob), value)\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits, value = self.forward(state_tensor)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Sample action\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        action_prob = probs[0, action].item()\n",
    "\n",
    "        return (action.item(), action_prob), value.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = [] # Extra in PPO\n",
    "        self.probs = []\n",
    "        self.dones = []\n",
    "\n",
    "    def add(self, state: np.array, action: int, reward: float, value: float, prob: float, done: bool):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.probs.append(prob)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def _compute_returns(self, gamma: float = 0.99) -> List[float]:\n",
    "        returns = []\n",
    "        G = 0\n",
    "\n",
    "        for r, d in zip(reversed(self.rewards), reversed(self.dones)):\n",
    "            G = r + gamma * G * (1 - int(d))\n",
    "            returns.append(G)\n",
    "\n",
    "        return list(reversed(returns))\n",
    "\n",
    "    def _compute_advantages(self, gamma: float = 0.99, decay: float = 0.97):\n",
    "        \"\"\"Computes GAE for the trajectory\"\"\"\n",
    "        next_values = np.concatenate([self.values[1:], [0]])\n",
    "        deltas = [reward + gamma * next_value - value for (reward, value, next_value) in zip(self.rewards, self.values, next_values)]\n",
    "\n",
    "        gaes = [deltas[-1]]\n",
    "        for i in reversed(range(len(deltas) - 1)):\n",
    "            gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "        return np.array(gaes[::-1])\n",
    "\n",
    "    def get_return(self, gamma: float) -> float:\n",
    "        \"\"\"Returns return (not reward) for the entire trajectory\"\"\"\n",
    "        returns = self._compute_returns(gamma)\n",
    "        # If you do sum, numbers would go wild based on length of the game when _compute_returns is applied\n",
    "        # Mean is larger if game was larger, but the differences aren't wild like when summing\n",
    "        return np.mean(returns)\n",
    "\n",
    "    def to_tensor(self, gamma: float = 0.99):\n",
    "        \"\"\"Returns states, actions, values, log_probs, probs, returns\"\"\"\n",
    "        states = torch.FloatTensor(np.array(self.states))\n",
    "        actions = torch.LongTensor(np.array(self.actions))\n",
    "        probs = torch.stack([torch.tensor(p) for p in self.probs])\n",
    "        returns = torch.FloatTensor(self._compute_returns(gamma)) # instead of reward\n",
    "\n",
    "        return states, actions, probs, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.90099501, 3.9403989999999998, 2.9701, 1.99, 1.0] [3.73036137 2.85365133 1.940697   0.99       0.        ] 2.960298802\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "t = Trajectory()\n",
    "t.add(state=np.array([0, 1]), action=0, reward=1, value=1, prob=0.2, done=False)\n",
    "t.add(state=np.array([0, 1]), action=1, reward=1, value=1, prob=0.2, done=False)\n",
    "t.add(state=np.array([0, 1]), action=0, reward=1, value=1, prob=0.2, done=False)\n",
    "t.add(state=np.array([0, 1]), action=0, reward=1, value=1, prob=0.2, done=False)\n",
    "t.add(state=np.array([0, 1]), action=1, reward=1, value=1, prob=0.2, done=True)\n",
    "\n",
    "gamma = 0.99\n",
    "print(t._compute_returns(gamma), t._compute_advantages(), t.get_return(gamma))\n",
    "\n",
    "states, actions, probs, returns = t.to_tensor()\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, env: gym.Env,\n",
    "                 hidden_dim: int = 64, gamma: float = 0.99,\n",
    "                 clip_param: float = 0.2, target_kl_div: float = 0.01,\n",
    "                 max_policy_train_iters: int = 80, max_value_train_iters: int = 80,\n",
    "                 policy_lr=3e-4, value_lr=1e-2):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.clip_param = clip_param\n",
    "        self.target_kl_div = target_kl_div\n",
    "\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.max_value_train_iters = max_value_train_iters\n",
    "\n",
    "        self.model = PolicyAndValueNetwork(input_dim=self.state_dim, hidden_dim=hidden_dim, policy_output_dim=self.action_dim)\n",
    "\n",
    "        policy_params = list(self.model.shared_layers.parameters()) + list(self.model.policy_layers.parameters())\n",
    "        self.policy_optim = Adam(policy_params, lr=policy_lr)\n",
    "\n",
    "        value_params = list(self.model.shared_layers.parameters()) + list(self.model.value_layers.parameters())\n",
    "        self.value_optim = Adam(value_params, lr=value_lr)\n",
    "\n",
    "    def collect_trajectories(self, n_trajectories: int) -> List[Trajectory]:\n",
    "        trajectories = []\n",
    "\n",
    "        for _ in range(n_trajectories):\n",
    "            traj = Trajectory()\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                (action, action_prob), value = self.model.get_action_and_value(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                traj.add(state=state, action=action, reward=reward, value=value, prob=action_prob, done=done)\n",
    "                state = next_state\n",
    "\n",
    "            trajectories.append(traj)\n",
    "\n",
    "        return trajectories\n",
    "\n",
    "    def update_policy(self, states: torch.Tensor, actions: torch.Tensor, old_log_probs: torch.Tensor, advantages: torch.Tensor):\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "            self.policy_optim.zero_grad()\n",
    "\n",
    "            new_logits = self.model.policy(states)\n",
    "            new_dist = Categorical(logits=new_logits)\n",
    "            new_log_probs = new_dist.log_prob(actions) # log probs for the same action\n",
    "\n",
    "            # Derivation: e^(new_log_prob-old_log_prob) (efficient to calculate) => e^new_log_prob / e^old_log_prob => new_prob / old_prob (original formula)\n",
    "            policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = policy_ratio.clamp(1 - self.clip_param, 1 + self.clip_param)\n",
    "\n",
    "            full_loss = policy_ratio * advantages # without clipping\n",
    "            clipped_loss = clipped_ratio * advantages # with clipping\n",
    "\n",
    "            # mean flattens out the matrix and - sign is required because pytorch is supposed to minimize, not maximize\n",
    "            policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "\n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_div = (old_log_probs - new_log_probs).mean().abs() # FIXME: Shouldn't this have .abs() in the end?\n",
    "            if kl_div >= self.target_kl_div:\n",
    "                # print(\"large KL div\", kl_div)\n",
    "                # Heavy deviations from the original thing. Early stop here\n",
    "                break\n",
    "\n",
    "    def update_value(self, states: torch.Tensor, returns: torch.Tensor):\n",
    "        # TODO: Allow training with multiple trajectories at once?\n",
    "        for _ in range(self.max_value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "\n",
    "            values = self.model.value(states).reshape(-1, )\n",
    "            value_loss = ((returns - values) ** 2) # Simple L2 loss\n",
    "            value_loss = value_loss.mean()\n",
    "\n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "    def train(self, n_episodes: int):\n",
    "        rewards_history = []\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            traj = self.collect_trajectories(n_trajectories=1)[0]\n",
    "\n",
    "            # Just for logging:\n",
    "            episode_reward = sum(traj.rewards)\n",
    "            rewards_history.append(episode_reward)\n",
    "\n",
    "            # TODO: Allow training with multiple trajectories at once?\n",
    "            states, actions, probs, returns = traj.to_tensor()\n",
    "\n",
    "            advantages = torch.FloatTensor(traj._compute_advantages())\n",
    "            calculated_log_probs = -torch.log(1 / probs)\n",
    "\n",
    "            self.update_policy(states=states, actions=actions, old_log_probs=calculated_log_probs, advantages=advantages)\n",
    "            self.update_value(states=states, returns=returns)\n",
    "\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(\"Episode {}, Episode reward: {:.2f}\".format(episode+1, episode_reward))\n",
    "\n",
    "        return rewards_history\n",
    "\n",
    "    def evaluate(self, env: gym.Env, n_episodes: int = 10, render: bool = False):\n",
    "        rewards = []\n",
    "\n",
    "        for _ in range(n_episodes):\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                (action, _), _ = self.model.get_action_and_value(state) # TODO: Add .get_action for perf\n",
    "                state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "\n",
    "        avg_reward = np.mean(rewards)\n",
    "        print(\"Evaluation: Average reward over {} episodes: {:.2f}\".format(n_episodes, avg_reward))\n",
    "\n",
    "        return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init ppo algorithm and env\n",
    "\n",
    "ENV_NAME = 'CartPole-v1' # Possible values: CartPole-v1, Acrobot-v1\n",
    "\n",
    "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
    "ppo = PPO(env=env, clip_param=0.2, target_kl_div=0.02, max_policy_train_iters=40, max_value_train_iters=40, policy_lr=3e-4, value_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 4]) torch.Size([37]) torch.Size([37]) torch.Size([37])\n",
      "tensor([0.5348, 0.4688, 0.5347, 0.5311, 0.4718, 0.4691, 0.4659, 0.5347, 0.4658,\n",
      "        0.4652, 0.5313, 0.5353, 0.4653, 0.5354, 0.5349, 0.4685, 0.5348, 0.5315,\n",
      "        0.4716, 0.5312, 0.4718, 0.5308, 0.4721, 0.5303, 0.4723, 0.5298, 0.4726,\n",
      "        0.5292, 0.5272, 0.4713, 0.5270, 0.4713, 0.5270, 0.4710, 0.5275, 0.5295,\n",
      "        0.4677])\n",
      "tensor([31.0551, 30.3587, 29.6552, 28.9447, 28.2269, 27.5020, 26.7697, 26.0300,\n",
      "        25.2828, 24.5281, 23.7657, 22.9957, 22.2179, 21.4322, 20.6386, 19.8369,\n",
      "        19.0272, 18.2093, 17.3831, 16.5486, 15.7057, 14.8542, 13.9942, 13.1254,\n",
      "        12.2479, 11.3615, 10.4662,  9.5618,  8.6483,  7.7255,  6.7935,  5.8520,\n",
      "         4.9010,  3.9404,  2.9701,  1.9900,  1.0000])\n",
      "[-0.12627637386322021, -0.13448408246040344, -0.12569132447242737, -0.1344195306301117, -0.14513495564460754, -0.13459813594818115, -0.12454517185688019, -0.12022320926189423, -0.12495197355747223, -0.12017303705215454, -0.11208745837211609, -0.11985857039690018, -0.12564124166965485, -0.11967506259679794, -0.12542150914669037, -0.13397249579429626, -0.12475750595331192, -0.13383199274539948, -0.1440753936767578, -0.13343659043312073, -0.14440375566482544, -0.13332082331180573, -0.14489081501960754, -0.1335231065750122, -0.14582037925720215, -0.1340809464454651, -0.14698070287704468, -0.13497044146060944, -0.14833946526050568, -0.1680886447429657, -0.15081295371055603, -0.17123056948184967, -0.15405245125293732, -0.17507794499397278, -0.1582188606262207, -0.18007618188858032, -0.20332522690296173]\n"
     ]
    }
   ],
   "source": [
    "t = ppo.collect_trajectories(1)[0]\n",
    "\n",
    "states, actions, probs, returns = t.to_tensor()\n",
    "\n",
    "print(states.shape, actions.shape, probs.shape, returns.shape)\n",
    "\n",
    "print(probs)\n",
    "print(returns)\n",
    "print(t.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Avg reward: 87.00\n",
      "Episode 20, Avg reward: 18.00\n",
      "Episode 30, Avg reward: 132.00\n",
      "Episode 40, Avg reward: 144.00\n",
      "Episode 50, Avg reward: 258.00\n",
      "Episode 60, Avg reward: 292.00\n",
      "Episode 70, Avg reward: 173.00\n",
      "Episode 80, Avg reward: 253.00\n",
      "Episode 90, Avg reward: 315.00\n",
      "Episode 100, Avg reward: 293.00\n",
      "Episode 110, Avg reward: 500.00\n",
      "Episode 120, Avg reward: 229.00\n",
      "Episode 130, Avg reward: 378.00\n",
      "Episode 140, Avg reward: 500.00\n",
      "Episode 150, Avg reward: 500.00\n",
      "Episode 160, Avg reward: 304.00\n",
      "Episode 170, Avg reward: 500.00\n",
      "Episode 180, Avg reward: 335.00\n",
      "Episode 190, Avg reward: 500.00\n",
      "Episode 200, Avg reward: 500.00\n",
      "Episode 210, Avg reward: 500.00\n",
      "Episode 220, Avg reward: 374.00\n",
      "Episode 230, Avg reward: 231.00\n",
      "Episode 240, Avg reward: 500.00\n",
      "Episode 250, Avg reward: 332.00\n",
      "Episode 260, Avg reward: 388.00\n",
      "Episode 270, Avg reward: 275.00\n",
      "Episode 280, Avg reward: 267.00\n",
      "Episode 290, Avg reward: 340.00\n",
      "Episode 300, Avg reward: 486.00\n",
      "Evaluation: Average reward over 10 episodes: 488.80\n"
     ]
    }
   ],
   "source": [
    "rewards = ppo.train(n_episodes=300)\n",
    "avg_reward = ppo.evaluate(env, n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: Average reward over 1 episodes: 380.00\n"
     ]
    }
   ],
   "source": [
    "## Evaluate:\n",
    "env = gym.make(ENV_NAME, max_episode_steps=500, render_mode='human')\n",
    "_ = ppo.evaluate(env, 1, True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-fSO1OvS5-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
